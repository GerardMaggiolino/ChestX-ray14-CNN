{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Description: \n",
    "# Trainer for ArchOneCNN, performing k-fold validation and undersampling of the\n",
    "# training set. \n",
    "#  \n",
    "# Cell 1: \n",
    "# Set up of model hyperparameters and dataset. \n",
    "# Cell 2: \n",
    "# K-fold training procedure. \n",
    "# Cell 3: \n",
    "# Testing of model on partitioned test set.  \n",
    "# Cell 4: \n",
    "# Evaluation of results. \n",
    "# \n",
    "################################################################################\n",
    "from arch1_cnn import *\n",
    "from arch1_cnn import ArchOneCNN\n",
    "\n",
    "# Constants \n",
    "CATEGORIES = 14                    # Number of categories in classification \n",
    "MODEL_NAME = \"Architecture-1 CNN\"  # Model name for graphing \n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 5              # Number of full passes through the dataset\n",
    "batch_size = 16             # Number of samples in each minibatch\n",
    "learning_rate = 0.0001  \n",
    "seed = np.random.seed(1)    # Seed the random number generator for reproducibility\n",
    "p_test = 0.2                # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "# Training Variables \n",
    "N = 140                     # Number of minibatchs before storing metrics / performing validation (getting data point)\n",
    "early_stop = True           # Perform early stopping if increase on validation set loss \n",
    "early_stop_evals = 7        # How many consecutive increases in loss before ending training\n",
    "k = 2                       # Number of folds for validation \n",
    "\n",
    "# Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "transform = transforms.Compose([transforms.Resize([512,512]), transforms.ToTensor(), ])\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the k-validation sets and test set, addresses class imbalance by training set manipulation  \n",
    "k_loader, test_loader = create_k_loaders(batch_size, seed, transform=transform, \n",
    "                                                             k=k, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n",
    "\n",
    "# Instantiate an ArchOneCNN to run on the GPU or CPU based on CUDA support\n",
    "model = ArchOneCNN()\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "# Define the loss criterion and instantiate the gradient descent optimizer\n",
    "criterion = nn.BCELoss() # loss criteria are defined in the torch.nn package\n",
    "\n",
    "#TODO: Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # optimizers are defined in the torch.optim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "\n",
    "# Holds training accuracy for k-sets, loss averaged over last N minibatches\n",
    "training_acc = [[] for i in range(k)]\n",
    "training_loss = [[] for i in range(k)]\n",
    "# Hold validation accuracy, loss for total set after each N minibatches\n",
    "validation_acc = [[] for i in range(k)]\n",
    "validation_loss = [[] for i in range(k)]\n",
    "# Holds number of data points collected \n",
    "num_data_points = [0 for i in range(k)]\n",
    "\n",
    "# Use kset as validation for each k-fold \n",
    "for kset in range(k): \n",
    "    isDone = False     # Set by early stop \n",
    "    loss_increase = 0  # Tracks number of validations where loss increases\n",
    "\n",
    "    # Begin training procedure \n",
    "    for epoch in range(num_epochs):\n",
    "        if isDone:\n",
    "            break\n",
    "        N_minibatch_acc = N_minibatch_loss = 0.0    \n",
    "        # Iterate over each loader in the kset, using as training set\n",
    "        for t_number, train_loader in enumerate(k_loader[:kset] + k_loader[kset + 1:], 0):\n",
    "            # Get the next minibatch of images, labels for training\n",
    "            for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "                \n",
    "                # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "                images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "                # Zero out the stored gradient (buffer) from the previous iteration\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Perform the forward pass through the network and compute the loss\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Update accuracy and loss for this one minibatch \n",
    "                correct = ((outputs > 0.5) == labels.byte()).sum().item()\n",
    "                N_minibatch_acc += correct / (batch_size * CATEGORIES)\n",
    "                N_minibatch_loss += loss.item()\n",
    "\n",
    "                # Calculate loss and accuracy over past N minibatches, then validate \n",
    "                if (minibatch_count + (t_number * len(train_loader))) % N == 0:\n",
    "                    num_data_points[kset] += 1\n",
    "                    # For the first minibatch, don't divide by N since it's one minibatch \n",
    "                    if (minibatch_count + (t_number * len(train_loader))) != 0: \n",
    "                        N_minibatch_acc /= N \n",
    "                        N_minibatch_loss /= N\n",
    "\n",
    "                    # Add the averaged and accuracy for training set\n",
    "                    training_acc[kset].append(N_minibatch_acc)\n",
    "                    training_loss[kset].append(N_minibatch_loss)\n",
    "\n",
    "                    print(f'K-Set {kset + 1}, Epoch {epoch + 1}: Minibatch ' + \n",
    "                          f'{minibatch_count + (t_number * len(train_loader))} processed. ' +\n",
    "                          f'Accuracy, Loss: {round(N_minibatch_acc, 4)}, {round(N_minibatch_loss, 2)}')\n",
    "                    # Reset metric for next batch of minibatches \n",
    "                    N_minibatch_acc = N_minibatch_loss = 0.0 \n",
    "\n",
    "                    # Validation\n",
    "                    val_loss = 0.0\n",
    "                    val_correct = 0.0\n",
    "                    # Sum total correct and loss over validation set \n",
    "                    with torch.no_grad(): \n",
    "                        for val_images, val_labels in k_loader[kset]:\n",
    "                            # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "                            val_images, val_labels = val_images.to(computing_device), val_labels.to(computing_device)\n",
    "                            # Perform the forward pass through the network and compute the loss and correct \n",
    "                            val_outputs = model(val_images)\n",
    "                            val_loss += criterion(val_outputs, val_labels).item()  \n",
    "                            val_correct += ((val_outputs > 0.5) == val_labels.byte()).sum().item()\n",
    "                        # Add metrics to list \n",
    "                    validation_acc[kset].append(val_correct / (len(k_loader[kset]) * batch_size * CATEGORIES))\n",
    "                    validation_loss[kset].append(val_loss / len(k_loader[kset]))  \n",
    "                    print(f'\\tValidation Accuracy, Loss: {round(validation_acc[kset][-1], 4)}, {round(validation_loss[kset][-1], 5)}')\n",
    "                    # Perform validation loss check to early stop \n",
    "                    if early_stop:\n",
    "                        if min(validation_loss[kset]) < validation_loss[kset][-1]: \n",
    "                            # Save state of model as soon as loss goes up \n",
    "                            if loss_increase == 0: \n",
    "                                torch.save(model.state_dict(), f'Lowest_Loss_{Model_Name}_K{kset + 1}.pt')\n",
    "                            loss_increase += 1\n",
    "                        else:\n",
    "                            loss_increase = 0\n",
    "                        # Save model state where loss started increasing, return \n",
    "                        if loss_increase >= early_stop_evals: \n",
    "                            print(f'\\nEarly Stop at Epoch {epoch}: Minibatch {minibatch_count + (t_number * len(train_loader))}')\n",
    "                            print(f'\\tBest Model Saved for K{kset}, {early_stop_evals * N} Minibatches Prior\\n')\n",
    "                            isDone = True\n",
    "                            break\n",
    "                # End loss, accuracy, and validation of past N minibatches \n",
    "\n",
    "                if not isDone: \n",
    "                    # Automagically compute the gradients and backpropagate the loss through the network\n",
    "                    loss.backward()\n",
    "                    # Update the weights\n",
    "                    optimizer.step()\n",
    "            # End iteration of ksets for k-validation \n",
    "            if isDone:\n",
    "                break\n",
    "        # End iteration of minibatches (1 epoch) \n",
    "    # End processing of all epochs \n",
    "# End processing of all ksets \n",
    "\n",
    "# Retrieve the model with the lowest loss over ksets, save only its information \n",
    "lowest_loss = [min(l_) for l_ in validation_loss]\n",
    "best_k = lowest_loss.index(min(lowest_loss))\n",
    "\n",
    "# Always saves lowest loss model, regardless of early stopping \n",
    "print(f'Retrieving K{best_k + 1} Model, Loss of: {min(lowest_loss)}')\n",
    "model.load_state_dict(torch.load(f'Lowest_Loss_{Model_Name}_K{best_k + 1}.pt'))\n",
    "training_acc = training_acc[best_k]\n",
    "training_loss = training_loss[best_k]\n",
    "validation_acc = validation_acc[best_k]\n",
    "validation_loss = validation_loss[best_k]\n",
    "num_data_points = num_data_points[best_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing over test set \n",
    "\n",
    "# Class based performance, stores total values over test set \n",
    "true_pos = torch.zeros(CATEGORIES, dtype=torch.int64, device=computing_device)\n",
    "false_pos = true_pos.clone()\n",
    "false_neg = true_pos.clone()\n",
    "total_corr = true_pos.clone()\n",
    "confusion_matrix = torch.zeros((CATEGORIES, CATEGORIES), dtype=torch.float64, device=computing_device)\n",
    "# Used to prevent divide by zero \n",
    "epsilon = 0.00000001\n",
    "# Decimal Precision Desired \n",
    "decimal_prec = 6\n",
    "# Holds first 3 batches of classification \n",
    "batch_classification = [] \n",
    "\n",
    "print(\"Testing Class Weighted Performance ...\")\n",
    "for batch, (images, labels) in enumerate(test_loader, 0):\n",
    "    # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "    images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "    \n",
    "    # Find sigmoid labelled classification \n",
    "    prediction = (model(images) > 0.5)\n",
    "    \n",
    "    # Cast for ease of calculations \n",
    "    prediction = prediction.type(torch.int8)\n",
    "    \n",
    "    # Save first 3 batches of classification \n",
    "    if batch < 3: \n",
    "        batch_classification.append(prediction.int().cpu().numpy().tolist())\n",
    "        \n",
    "    labels = labels.type(torch.int8)\n",
    "    # Calculate peformance over batch, adding in place to totals \n",
    "    true_pos.add_(torch.sum(prediction + labels == 2, dim=0))\n",
    "    false_pos.add_(torch.sum(prediction - labels == 1, dim=0))\n",
    "    false_neg.add_(torch.sum(prediction - labels == -1, dim=0))\n",
    "    total_corr.add_(torch.sum(prediction == labels, dim=0))\n",
    "    \n",
    "    # Calculate confusion matrix \n",
    "    # Add correctly activated elements to confusion matrix diagonal \n",
    "    confusion_matrix.add_(torch.diag(torch.sum(prediction + labels == 2, dim=0)).double())\n",
    "    # Add incorrectly activated elements, per example \n",
    "    conf = (labels - prediction == 1).double()\n",
    "    for i in range(len(conf)): \n",
    "        confusion_matrix.add_(torch.matmul(\n",
    "            torch.reshape(conf[i].double(), (CATEGORIES, 1)), \n",
    "            torch.reshape(\n",
    "                prediction[i].double() / (torch.sum(prediction[i].double()) + epsilon), (1, CATEGORIES))))\n",
    "    \n",
    "# Convert tensors to cpu numpy arrays to access elements \n",
    "true_pos = true_pos.cpu().numpy()\n",
    "false_pos = false_pos.cpu().numpy()\n",
    "false_neg = false_neg.cpu().numpy()\n",
    "total_corr = total_corr.cpu().numpy()\n",
    "confusion_matrix = confusion_matrix.cpu().numpy().tolist()\n",
    "\n",
    "# Calculate performance metrics per class \n",
    "performance = {'Accuracy': total_corr / (len(test_loader) * batch_size), \n",
    "               'Precision': true_pos / (false_pos + true_pos + epsilon), \n",
    "               'Recall': true_pos / (true_pos + false_neg + epsilon)}\n",
    "performance['BCR'] = (performance['Precision'] + performance['Recall']) / 2.0\n",
    "\n",
    "# Calculate aggregated performance metrics \n",
    "agg_performance = {'Aggregated ' + k: sum(v) / CATEGORIES for k, v in performance.items()}\n",
    "\n",
    "# Convert to lists and round \n",
    "for k, v in performance.items(): \n",
    "    performance[k] =  [round(i, decimal_prec) for i in v]\n",
    "for k, v in agg_performance.items(): \n",
    "    agg_performance[k] = round(float(v), decimal_prec)\n",
    "for r in range(len(confusion_matrix)): \n",
    "    for c in range(len(confusion_matrix[0])): \n",
    "        confusion_matrix[r][c] = round(confusion_matrix[r][c], decimal_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying graphs and metrics \n",
    "\n",
    "# Display filters from first 3 layers \n",
    "all_filters = [model.conv1_1.weight.data.cpu().numpy(),\n",
    "               model.conv2_1.weight.data.cpu().numpy(), \n",
    "               model.conv3_1.weight.data.cpu().numpy()]\n",
    "# For each layer \n",
    "for l, layer in enumerate(all_filters, 1):\n",
    "    # Get two filters (second and third filter of layer) \n",
    "    # PyTorch indexs layers as such: (input channel, subfilter, dim, dim)\n",
    "    for filt_index in range(1, 3): \n",
    "        _, ax = plt.subplots()\n",
    "        # Get second and third filter, first subfilter \n",
    "        ax.imshow(layer[filt_index][0])\n",
    "        ax.set_title(f'{MODEL_NAME} - Layer {l}: Filter {filt_index}')\n",
    "        plt.show()\n",
    "\n",
    "# Plot accuracy for training and validation \n",
    "batch_range = [i for i in range(num_data_points)]\n",
    "line1, = plt.plot(batch_range[1:], validation_acc[1:], 'g', label=\"Validation\")\n",
    "line2, = plt.plot(batch_range[1:], training_acc[1:], 'b', label=\"Training\")\n",
    "plt.legend(handles=[line1, line2])\n",
    "plt.xlabel(f'{N} Minibatches')\n",
    "plt.ylabel('Percent Correctly Classified')\n",
    "plt.title(f'{MODEL_NAME} - Average Accuracy over Past {N} Minibatches')\n",
    "plt.show()\n",
    "# Plot loss for training and validation \n",
    "line1, = plt.plot(batch_range[1:], validation_loss[1:], 'g', label=\"Validation\")\n",
    "line2, = plt.plot(batch_range[1:], training_loss[1:], 'b', label=\"Training\")\n",
    "plt.legend(handles=[line1, line2])\n",
    "plt.xlabel(f'{N} Minibatches')\n",
    "plt.ylabel('Canonical Loss')\n",
    "plt.title(f'{MODEL_NAME} - Loss over Past {N} Minibatches')\n",
    "plt.show()\n",
    "\n",
    "# Print final test data \n",
    "print('RAW BY CLASS')\n",
    "print(f'\\tTrue Positives\\n{true_pos}')\n",
    "print(f'\\tFalse Positives\\n{false_pos}')\n",
    "print(f'\\tFalse Negatives\\n{false_neg}') \n",
    "print(f'\\tTotal Correct\\n{total_corr}')\n",
    "print() \n",
    "\n",
    "print('CALCULATED METRICS BY CLASS')\n",
    "for k, v in performance.items():\n",
    "    print(f'\\t{k}')\n",
    "    print(v)\n",
    "print()\n",
    "\n",
    "print('CALCULATED METRICS EVENLY WEIGHTED')\n",
    "for k, v in agg_performance.items():\n",
    "    if k == 'Aggregated BCR':\n",
    "        print(f'{k}:\\t\\t{v}')\n",
    "    else: \n",
    "        print(f'{k}:\\t{v}')\n",
    "print()\n",
    "\n",
    "print('CONFUSION MATRIX VALUES BY ROW')\n",
    "for row in confusion_matrix: \n",
    "    print(row) \n",
    "print() \n",
    "\n",
    "# Shows example classifications, this is what our model is putting out \n",
    "print('FIRST 3 BATCHES OF CLASSIFICATIONS FROM TEST')\n",
    "for i in range(len(batch_classification)):\n",
    "    print(f'Classification #{i + 1}')\n",
    "    for row in batch_classification[i]: \n",
    "        print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
